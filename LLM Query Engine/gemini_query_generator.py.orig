"""
Gemini LLM Query Generator - A module to convert natural language to SQL using Google Gemini (1.5 Flash)
"""
import os
import pandas as pd
import datetime
import json
import os
import logging
import pandas as pd
from typing import Dict, List, Any, Optional
from token_logger import TokenLogger

# Import RAG components
try:
    from milvus_setup.rag_embedding import MultiClientMilvusRAG
    from milvus_setup.schema_processor import process_schema_aware_data
    rag_available = True
except ImportError:
    rag_available = False
    logging.warning("RAG components not available. Using standard dictionary processing.")

# Import Google GenerativeAI (Gemini) SDK
gemini_available = False
genai_with_token_counter = False
try:
    import google.generativeai as genai
    gemini_available = True
    try:
        # Check if token counter is available
        from google.genai.types import HttpOptions
        genai_with_token_counter = True
    except ImportError:
        logging.info("Google GenerativeAI SDK installed but doesn't support token counting")
except ImportError:
    logging.warning("Google GenerativeAI SDK not installed. Please install with: pip install google-generativeai")

# Reuse these functions from the OpenAI implementation
from llm_query_generator import load_data_dictionary, format_data_dictionary

def generate_sql_prompt(tables: List[Dict[str, Any]], query: str, limit_rows: int = 100, include_charts: bool = False, rag_context: str = "") -> str:
    """
    Generate system prompt for Gemini with table schema information
    """
    tables_context = ""
    for table in tables:
        if table['schema']:
            tables_context += f"Table: {table['name']} (Schema: {table['schema']})\n"
        else:
            tables_context += f"Table: {table['name']}\n"
        if table['description']:
            tables_context += f"Description: {table['description']}\n"
        tables_context += "Columns:\n"
        for col in table['columns']:
            pk_indicator = " (PRIMARY KEY)" if col.get('is_primary_key') else ""
            fk_info = ""
            if col.get('is_foreign_key'):
                fk_info = f" (FOREIGN KEY to {col.get('foreign_key_table')}.{col.get('foreign_key_column')})"
            tables_context += f"  - {col['name']} ({col['type']}){pk_indicator}{fk_info}: {col['description']}"
            if col['business_name'] != col['name']:
                tables_context += f" [Business Name: {col['business_name']}]"
            tables_context += "\n"
        tables_context += "\n"
    # Create chart instructions if needed
    chart_instructions = """

After generating the SQL query, you must also recommend appropriate chart types for visualizing the results. Follow these rules for chart recommendations:

1. Analyze the query structure to understand what data will be returned
2. For single numeric values KPI_CARD: 
   Display as minimal cards with bold label at top, large formatted number below, no icons, clean white background, centered text only.
3. For purely categorical data with no numeric measures, recommend a table visualization.
4. Consider specialized chart types for distribution analysis:

  **Histogram Charts**: Distribution analysis of continuous variables
        - Required: Numerical continuous data only (no categorical)
        - Best for: Frequency/spread/skewness/outliers in large datasets
        - Use auto-binning (Sturges/Freedman-Diaconis) for proper bin sizing
        - X-axis: value range,
        - "y_axis": [],   "y_axis": null,  // âœ… No column needed - frequency is calculated
        For a histogram:
        - Use 1 numeric column.
        - X-axis = value bins from that column.
        - Y-axis = count (frequency), computed from how many values fall in each bin.
        - Y-axis is not from any column.
        - Ensure contiguous bins (no gaps)
        - Avoid overlapping distributions (use separate plots/density plots)
        - Skip for small datasets (use box/dot plots instead)

        **Box Plot Charts**: Distribution comparison between groups
        - Required: Numerical data (can group by categorical)
          if one columns then use null for x_axis,
          other wise categorical column for x_axis
           e.g:  "x_axis": "PRODUCT_CATEGORY",
            "y_axis": ["SALES_AMOUNT"],
            "color_by": "PRODUCT_CATEGORY",
            - Best for: Comparing distributions, showing central tendency/spread/outliers
            - Box = IQR (Q1-Q3), line = median
            - Whiskers = Q1-1.5Ã—IQR to Q3+1.5Ã—IQR
            - Points beyond whiskers = outliers
            - Best for side-by-side comparisons
            - Consider combining with histograms/violin plots for distribution shape details

5. Recommend 1-3 appropriate chart types (bar, line, pie, scatter, histogram, boxplot, KPI_CARD, MIX etc.) based on the query's structure
6. For each recommendation, provide:
   - chart_type: The type of chart (bar, line, pie, scatter,mix, etc.)
   - reasoning: Brief explanation of why this chart type is appropriate
   - priority: Importance ranking (1 = highest)
   - chart_config: Detailed configuration including:
     * title: Descriptive chart title
     * x_axis: Column to use for x-axis
     * y_axis: Column to use for y-axis
     * color_by: Column to use for segmentation/colors (if applicable)
     * aggregate_function: Any aggregation needed (SUM, AVG, etc.)
     * chart_library: Recommended visualization library (plotly)
     * additional_config: Other relevant settings like orientation, legend, etc.

6. Also provide 2-3 data insights that would be valuable to highlight

Your response must be a valid JSON object with the following structure:
{
  "sql": "YOUR SQL QUERY HERE",
  "chart_recommendations": [
    {
      "chart_type": "bar|pie|line|scatter|mix|kpi_card|table|etc",
      "reasoning": "Why this chart is appropriate",
      "priority": 1,
      "chart_config": {
        "title": "Chart title",
        "x_axis": "column_name",
        "y_axis": "column_name",
        "color_by": "column_name",
        "aggregate_function": "NONE|SUM|AVG|etc",
        "chart_library": "plotly",
        "additional_config": {
          "show_legend": true,
          "orientation": "vertical|horizontal"
        }
      }
    }
  ],
  }
""" if include_charts else """
Return ONLY the SQL code without any other text or explanations.
"""

    prompt = f"""You are an expert SQL query generator for Snowflake database.

Your task is to convert natural language questions into valid SQL queries that can run on Snowflake.
Use the following data dictionary to understand the database schema:

{tables_context}
{rag_context}

When generating SQL:
1. Only generate SELECT queries.
2. Use proper Snowflake SQL syntax with fully qualified table names including schema (e.g., SCHEMA.TABLE_NAME)
3. For column selections:
   - Always list columns individually (never concatenate or combine columns till user not asking in prompt)
   - Use consistent column casing - uppercase for all column names
   - Format each column on a separate line with proper indentation
4. Include appropriate JOINs based only on the relationships defined in the schema metadata.
5. Only use tables and columns that exist in the provided schema.
6. For numeric values:
   - Use standard CAST() function for type conversions (e.g., CAST(field AS DECIMAL) or CAST(field AS NUMERIC))
   - When using GROUP BY, always apply aggregate functions (SUM, AVG, etc.) to non-grouped numeric fields
   - Example: SUM(CAST(SALES_AMOUNT AS NUMERIC)) AS TOTAL_SALES
   - ALWAYS use NULLIF() for divisions to prevent division by zero errors:
     * For percentage calculations: (new_value - old_value) / NULLIF(old_value, 0) * 100
     * For ratios: numerator / NULLIF(denominator, 0)
   - For sensitive calculations that must return specific values on zero division:
     * Use CASE: CASE WHEN denominator = 0 THEN NULL ELSE numerator/denominator END
7. Format results with consistent column naming:
   - For aggregations, use uppercase names (e.g., SUM(sales) AS TOTAL_SALES)
   - For regular columns, maintain original casing
8. CRITICAL: Follow these row limit rules EXACTLY:
   a. If the user explicitly specifies a number in their query (e.g., "top 5", "first 10"), use EXACTLY that number in the LIMIT clause
   b. Otherwise, limit results to {limit_rows} rows
   c. NEVER override a user-specified limit with a different number
9. SUPERLATIVE QUERY HANDLING:
   a. CRITICAL: For PLURAL nouns in queries like "which sales representatives sold most" - NEVER add LIMIT 1, return MULTIPLE results
   b. For SINGULAR nouns in queries like "which sales rep sold most" - ALWAYS add `ORDER BY [relevant_metric] DESC LIMIT 1`
   c. Explicitly check if words like "representatives", "products", "customers" (plural) are used
   d. Examples: "which sales rep sold most" â†’ ONE result (LIMIT 1), "which sales representatives sold most" â†’ MULTIPLE results (NO LIMIT 1)

Generate a SQL query for: {query}{chart_instructions}
"""
    return prompt

def generate_sql_query_gemini(api_key: str, prompt: str, model: str = "models/gemini-1.5-flash-latest", query_text: str = "", log_tokens: bool = True, include_charts: bool = False) -> Dict[str, Any]:
    """
    Generate SQL query using Google Gemini API (OpenAI-compatible result dict)
    """
    if not gemini_available:
        return {
            "sql": "SELECT 'Gemini SDK not installed' AS error_message",
            "model": model,
            "error": "Gemini SDK not installed",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "success": False,
            "results": None,
            "row_count": 0,
            "error_execution": "Gemini SDK not installed",
            "execution_time_ms": 0
        }
    logger = TokenLogger() if log_tokens else None
    start_time = datetime.datetime.now()
    try:
        genai.configure(api_key=api_key)
        generation_config = {
            "temperature": 0.1,
            "max_output_tokens": 4000
        }
        model_obj = genai.GenerativeModel(model)
        response = model_obj.generate_content(prompt, generation_config=generation_config)
        sql_query = "SELECT 'No SQL query was successfully extracted' AS error_message"
        if response and hasattr(response, 'text') and response.text:
            full_text = response.text.strip()
            if not full_text:
                return {
                    "sql": sql_query,
                    "model": model,
                    "error": "Empty response from Gemini",
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                    "success": False,
                    "results": None,
                    "row_count": 0,
                    "chart_recommendations": None,
                    "chart_error": None,
                    "error_execution": "Empty response from Gemini",
                    "execution_time_ms": (datetime.datetime.now() - start_time).total_seconds() * 1000
                }
            
            # Extract SQL based on whether charts are requested
            import re
            import json
            
            # Initialize chart_recommendations and chart_error
            chart_recommendations = None
            chart_error = None
            
            # Log model response summary at debug level
            logging.debug("\n==== GEMINI RAW RESPONSE ====\n")
            logging.debug(full_text[:500] + "..." if len(full_text) > 500 else full_text)
            logging.debug("=============================")
            
            # Try to get chart recommendations if requested
            if include_charts:
                # Try to parse the JSON response with SQL and chart recommendations
                try:
                    # Remove code blocks if present
                    cleaned_text = full_text
                    if cleaned_text.startswith("```json") and cleaned_text.endswith("```"):
                        cleaned_text = cleaned_text[7:-3].strip()
                    elif cleaned_text.startswith("```") and cleaned_text.endswith("```"):
                        cleaned_text = cleaned_text[3:-3].strip()
                    
                    # Look for JSON object markers
                    json_start = cleaned_text.find('{')
                    json_end = cleaned_text.rfind('}')
                    
                    if json_start >= 0 and json_end > json_start:
                        json_content = cleaned_text[json_start:json_end+1]
                        logging.debug("Found JSON content between { and }")
                    else:
                        json_content = cleaned_text
                        logging.debug("No JSON markers found, using cleaned text")
                    
                    # Parse the JSON response
                    try:
                        json_data = json.loads(json_content)
                        
                        logging.debug(f"Successfully parsed JSON with keys: {list(json_data.keys())}")
                        
                        # Extract SQL query and chart recommendations
                        sql_query = json_data.get("sql", "")
                        chart_recommendations = json_data.get("chart_recommendations", [])
                        
                        # If chart_recommendations exists but is empty, try looking for other formats
                        if not chart_recommendations and "charts" in json_data:
                            chart_recommendations = json_data.get("charts", [])
                    except json.JSONDecodeError as e:
                        logging.error(f"Error parsing JSON response from Gemini: {str(e)}")
                        # Fallback if JSON parsing fails
                        chart_recommendations = None
                        chart_error = "Cannot generate charts: error in JSON parsing"
                        sql_query = "SELECT 'Error in SQL generation: JSON parsing failed' AS error_message"
                        continue
                
                # Handle cases where response is malformed JSON but contains SQL
                try:
                    sql_json_match = re.search(r'"sql"\s*:\s*"(.+?)(?=",|"})', cleaned_text, re.DOTALL)
                    if sql_json_match:
                        sql_candidate = sql_json_match.group(1)
                        logging.debug(f"Extracted SQL from partial JSON: {sql_candidate[:50]}...")
                    else:
                        # Extract SQL using the regular approach
                        sql_candidate = full_text
                        # Prefer to extract from code blocks, but fallback to raw text
                        if "```" in full_text:
                            # Extract SQL from code blocks
                            code_blocks = re.findall(r'```(?:sql)?(.+?)```', full_text, re.DOTALL)
                            if code_blocks:
                                sql_candidate = code_blocks[0].strip()
                except Exception as extract_err:
                    logging.error(f"Error extracting SQL from response: {str(extract_err)}")
                    continue

            # Count tokens
            try:
                # Get token count from completion response
                completion_tokens = getattr(completion_response, 'total_tokens', 0)
                
                # Calculate total tokens
                total_tokens = prompt_tokens + completion_tokens
                logging.debug(f"Gemini token count - Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}")
            except Exception as token_err:
                logging.warning(f"Could not count tokens for Gemini: {str(token_err)}")
                # Fallback to estimate
                prompt_tokens = len(prompt) // 4
                completion_tokens = len(sql_query) // 4
                total_tokens = prompt_tokens + completion_tokens
                logging.debug(f"Gemini token ESTIMATE - Prompt: ~{prompt_tokens}, Completion: ~{completion_tokens}, Total: ~{total_tokens}")
        # Log SQL at info level
        logging.info("Generated SQL:")
        logging.info(sql)
